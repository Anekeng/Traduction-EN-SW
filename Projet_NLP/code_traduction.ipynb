{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\new_nlpevn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since emuchogu/swahili-english-translation couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\PC\\.cache\\huggingface\\datasets\\emuchogu___swahili-english-translation\\default\\0.0.0\\08b59ca54fb915d8547392c79c609029fc4c4d8e (last modified on Thu Mar  6 13:20:48 2025).\n"
     ]
    }
   ],
   "source": [
    "# Télécharger la base sur Hugging Face \n",
    "ds = load_dataset(\"emuchogu/swahili-english-translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'input', 'output'],\n",
       "        num_rows: 1115700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en DataFrame \n",
    "df = pd.DataFrame(ds[\"train\"])\n",
    "# Sauvegarder en fichier CSV\n",
    "df.to_csv(\"swahili_english_translation.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees = pd.read_csv(\"swahili_english_translation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = donnees.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.DataFrame(columns=[\"en\", \"ye\"])\n",
    "df_p[\"en\"] = df.apply(lambda row: row[\"input\"] if row.name % 2 == 0 else row[\"output\"], axis=1)\n",
    "df_p[\"ye\"] = df.apply(lambda row: row[\"output\"] if row.name % 2 == 0 else row[\"input\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1349</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>The people are outside.</td>\n",
       "      <td>Watu wako nje.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6272</td>\n",
       "      <td>8004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             en              ye\n",
       "count                     50000           50000\n",
       "unique                     1349            1343\n",
       "top     The people are outside.  Watu wako nje.\n",
       "freq                       6272            8004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>Mtu aliyepanda farasi anaruka juu ya ndege ili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>Mtu aliyepanda farasi anaruka juu ya ndege ili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>Watoto wakitabasamu na kutikisa kamera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>Watoto wakitabasamu na kutikisa kamera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>Mvulana anakimbia kwenye ubao wa kuteleza kati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>A man is sleeping.</td>\n",
       "      <td>Mwanamume fulani amelala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>A man is sleeping.</td>\n",
       "      <td>Mwanamume fulani amelala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>A man is sleeping.</td>\n",
       "      <td>Mwanamume fulani amelala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A man is sleeping.</td>\n",
       "      <td>Mwanamume fulani amelala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>A man is sleeping.</td>\n",
       "      <td>Mwanamume fulani amelala.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      A person on a horse jumps over a broken down a...   \n",
       "1      A person on a horse jumps over a broken down a...   \n",
       "2                  Children smiling and waving at camera   \n",
       "3                  Children smiling and waving at camera   \n",
       "4      A boy is jumping on skateboard in the middle o...   \n",
       "...                                                  ...   \n",
       "49995                                 A man is sleeping.   \n",
       "49996                                 A man is sleeping.   \n",
       "49997                                 A man is sleeping.   \n",
       "49998                                 A man is sleeping.   \n",
       "49999                                 A man is sleeping.   \n",
       "\n",
       "                                                      ye  \n",
       "0      Mtu aliyepanda farasi anaruka juu ya ndege ili...  \n",
       "1      Mtu aliyepanda farasi anaruka juu ya ndege ili...  \n",
       "2                 Watoto wakitabasamu na kutikisa kamera  \n",
       "3                 Watoto wakitabasamu na kutikisa kamera  \n",
       "4      Mvulana anakimbia kwenye ubao wa kuteleza kati...  \n",
       "...                                                  ...  \n",
       "49995                          Mwanamume fulani amelala.  \n",
       "49996                          Mwanamume fulani amelala.  \n",
       "49997                          Mwanamume fulani amelala.  \n",
       "49998                          Mwanamume fulani amelala.  \n",
       "49999                          Mwanamume fulani amelala.  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'échantillon d'entraînement: 32000\n",
      "Taille de l'échantillon de validation: 8000\n",
      "Taille de l'échantillon de test: 10000\n"
     ]
    }
   ],
   "source": [
    "# Convertir le DataFrame en une liste de dictionnaires\n",
    "data_list = df_p.to_dict(orient='records')\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement, validation et test\n",
    "train, test = train_test_split(data_list, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "train, validation = train_test_split(train, test_size=0.2, random_state=42)  # 80% train, 20% validation\n",
    "\n",
    "print(f\"Taille de l'échantillon d'entraînement: {len(train)}\")\n",
    "print(f\"Taille de l'échantillon de validation: {len(validation)}\")\n",
    "print(f\"Taille de l'échantillon de test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train)\n",
    "validation_dataset = Dataset.from_list(validation)\n",
    "test_dataset = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'ye'],\n",
      "        num_rows: 32000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'ye'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'ye'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Construire le DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_p[\"ye\"]\n",
    "pd.DataFrame(corpus).to_csv('corpus.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = df_p[\"en\"]\n",
    "pd.DataFrame(corpus2).to_csv('corpus2.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser un tokenizer basé sur WordPiece\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Ajouter des normalisations et des pré-tokenizers\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Configurer un entraîneur pour générer un vocabulaire\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000)\n",
    "\n",
    "# Charger un corpus et entraîner le tokenizer\n",
    "files = [\"corpus.txt\"]  \n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Définir un post-traitement \n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",
    ")\n",
    "# Sauvegarder le tokenizer\n",
    "tokenizer.save(\"TokenizerSW.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'anglais\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000)\n",
    "\n",
    "files = [\"corpus2.txt\"] \n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",
    ")\n",
    "tokenizer.save(\"TokenizerEN.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'M', '##tu', 'al', '##i', '##y', '##ep', '##and', '##a', 'far', '##a', '##si', 'an', '##ar', '##u', '##ka', 'ju', '##u', 'y', '##a', 'n', '##de', '##ge', 'il', '##i', '##y', '##o', '##v', '##un', '##j', '##ik', '##a', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tester le tokenizer\n",
    "encoded = tokenizer.encode(\"Mtu aliyepanda farasi anaruka juu ya ndege iliyovunjika.\")\n",
    "print(encoded.tokens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye_nlp = Tokenizer.from_file(\"TokenizerSW.json\")\n",
    "en_nlp = Tokenizer.from_file(\"TokenizerEN.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire Anglais: 2396\n",
      "Taille du vocabulaire Yemba: 2258\n"
     ]
    }
   ],
   "source": [
    "def yield_tokens(dataset, lang):\n",
    "    \"\"\"Générateur de tokens pour le vocabulaire.\"\"\"\n",
    "    for example in dataset:\n",
    "        yield example[lang].split()  # Tokenisation naïve par espaces\n",
    "\n",
    "# Construire le vocabulaire pour chaque langue\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "bos_token = \"<bos>\"\n",
    "eos_token = \"<eos>\"\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(train_data, \"en\"), specials= [unk_token, pad_token, bos_token, eos_token])\n",
    "ye_vocab = build_vocab_from_iterator(yield_tokens(train_data, \"ye\"), specials= [unk_token, pad_token, bos_token, eos_token])\n",
    "\n",
    "# Configurer l'index spécial pour <unk>\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "ye_vocab.set_default_index(ye_vocab[\"<unk>\"])\n",
    "\n",
    "# Vérification\n",
    "print(\"Taille du vocabulaire Anglais:\", len(en_vocab))\n",
    "print(\"Taille du vocabulaire Yemba:\", len(ye_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index de 'the': 14\n",
      "Index de 'anaruka': 551\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'indexation de mots\n",
    "print(\"Index de 'the':\", en_vocab[\"the\"])  # Retourne l'index si le mot existe, sinon <unk>\n",
    "print(\"Index de 'anaruka':\", ye_vocab[\"anaruka\"])  # Mot en swalli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<bos>',\n",
       " '<eos>',\n",
       " 'is',\n",
       " 'man',\n",
       " 'The',\n",
       " 'outside.',\n",
       " 'A',\n",
       " 'are']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<bos>',\n",
       " '<eos>',\n",
       " 'Mwanamume',\n",
       " 'nje.',\n",
       " 'huyo',\n",
       " 'Watu',\n",
       " 'wako',\n",
       " 'fulani']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ye_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ye_vocab[unk_token] == en_vocab[unk_token]\n",
    "assert ye_vocab[pad_token] == en_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "ye_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 128, 4, 170, 14, 594]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"our\", \"riparian\", \"field\", \"is\", \"near\", \"the\", \"bridge\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour convertir une phrase en indices\n",
    "def numericalize(sentence, vocab):\n",
    "    return [vocab[bos_token]] + [vocab[token] for token in sentence.split()] + [vocab[eos_token]]\n",
    "\n",
    "# Fonction de traitement d'un lot\n",
    "def collate_fn(batch):\n",
    "    en_batch = [torch.tensor(numericalize(item[\"en\"], en_vocab)) for item in batch]\n",
    "    ye_batch = [torch.tensor(numericalize(item[\"ye\"], ye_vocab)) for item in batch]\n",
    "    \n",
    "    en_batch = pad_sequence(en_batch, padding_value=en_vocab[pad_token])\n",
    "    ye_batch = pad_sequence(ye_batch, padding_value=ye_vocab[pad_token])\n",
    "    \n",
    "    return {\"en\": en_batch, \"ye\": ye_batch}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des chargeurs de données\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'encodeur\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du décodeur\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle initialisé!\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des hyperparamètres\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = len(en_vocab)\n",
    "output_dim = len(ye_vocab)\n",
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialisation du modèle\n",
    "encoder = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n",
    "decoder = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab[pad_token])\n",
    "\n",
    "print(\"Modèle initialisé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        src, trg = batch[\"en\"].to(device), batch[\"ye\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'évaluation\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src, trg = batch[\"en\"].to(device), batch[\"ye\"].to(device)\n",
    "            output = model(src, trg, 0)\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 1: Perte entraînement = 1.825, Perte validation = 1.197\n",
      "Époque 2: Perte entraînement = 0.751, Perte validation = 0.787\n",
      "Époque 3: Perte entraînement = 0.458, Perte validation = 0.637\n",
      "Époque 4: Perte entraînement = 0.385, Perte validation = 0.524\n",
      "Époque 5: Perte entraînement = 0.338, Perte validation = 0.513\n"
     ]
    }
   ],
   "source": [
    "# Boucle principale d'entraînement et d'évaluation\n",
    "epochs = 5\n",
    "clip = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    print(f\"Époque {epoch+1}: Perte entraînement = {train_loss:.3f}, Perte validation = {valid_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Données\n",
    "los = [1.825, 0.751, 0.458, 0.385, 0.338]\n",
    "val = [1.197, 0.787, 0.637, 0.524, 0.513]\n",
    "epochs = range(1, 6)\n",
    "\n",
    "# Tracé du graphique\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, los, label='perte entraînement', marker='o')\n",
    "plt.plot(epochs, val, label='perte validation', marker='s')\n",
    "\n",
    "# Ajout des labels et titre\n",
    "plt.xlabel(\"nombre d'épochs\")\n",
    "plt.ylabel(\"valeur de la perte\")\n",
    "plt.title(\"Évolution de la perte en fonction des épochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ion()\n",
    "plt.savefig(\"loss_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dossier pour sauvegarder le modèle\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé à l'époque 5 avec une perte de validation de 0.513\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': valid_loss,\n",
    "}, model_path)\n",
    "print(f\"Modèle sauvegardé à l'époque {epoch+1} avec une perte de validation de {valid_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle sauvegardé\n",
    "def load_model(model, optimizer, model_path=\"saved_models/best_model.pth\"):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Modèle chargé depuis l'époque {epoch} avec une perte de validation de {loss:.3f}\")\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé depuis l'époque 5 avec une perte de validation de 0.513\n"
     ]
    }
   ],
   "source": [
    "model_path=\"saved_models/best_model.pth\"\n",
    "model, optimizer = load_model(model, optimizer, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_vocab, ye_vocab, device, max_length=50):\n",
    "    \"\"\"Traduit une phrase de l'anglais vers le yemba.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenisation et conversion en indices\n",
    "    tokens = sentence.lower().split()\n",
    "    numericalized = [en_vocab[\"<bos>\"]]\n",
    "    \n",
    "    # Ajouter chaque token avec un fallback sur <unk> si le mot n'existe pas\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            numericalized.append(en_vocab[token])  # Tentative d'accès direct au vocabulaire\n",
    "        except KeyError:\n",
    "            numericalized.append(en_vocab[\"<unk>\"]) # Si le mot n'est pas trouvé, ajouter <unk>\n",
    "    numericalized.append(en_vocab[\"<eos>\"])\n",
    "    \n",
    "    # Conversion en tenseur PyTorch\n",
    "    src_tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Passage dans l'encodeur\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    \n",
    "    # Décodage itératif\n",
    "    trg_indexes = [ye_vocab[\"<bos>\"]]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == ye_vocab[\"<eos>\"]:\n",
    "            break\n",
    "  \n",
    "    trg_tokens = [ye_vocab.lookup_token(idx) for idx in trg_indexes]   # Conversion des indices en mots\n",
    "    \n",
    "    return \" \".join(trg_tokens[1:-1])  # Exclure <bos> et <eos>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = test_data['en'][:50]\n",
    "traduction = test_data['ye'][:50]\n",
    "df_phrases = pd.DataFrame({\"Phrases en anglais\": phrases, \"Phrases en swahili\": traduction})\n",
    "df_phrase = df_phrases.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The people are outside.', 'Watu wako nje.')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = test_data[1][\"en\"]\n",
    "expected_translation = test_data[1][\"ye\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction: Watu wako nje.\n"
     ]
    }
   ],
   "source": [
    "translation = translate_sentence(sentence, model, en_vocab, ye_vocab, device)\n",
    "print(\"Traduction:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction: Watu wako nje.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de traduction\n",
    "translation = translate_sentence(sentence, model, en_vocab, ye_vocab, device)\n",
    "print(\"Traduction:\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: The man is sitting.\n",
      "Réponse: Mwanamume huyo ameketi.\n",
      "Traduction: Mwanamume huyo ameketi.\n",
      "Phrase: The people are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: The woman is wearing white.\n",
      "Réponse: Mwanamke huyo amevaa mavazi meupe.\n",
      "Traduction: Mwanamke huyo amevaa mavazi meupe.\n",
      "Phrase: The people are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: A man is standing.\n",
      "Réponse: Mwanamume mmoja amesimama.\n",
      "Traduction: Mwanamume mmoja amesimama.\n",
      "Phrase: A man wearing a white shirt with black dot is holding a microphone as he stands in front of a background of black with white symbols on it.\n",
      "Réponse: Mwanamume aliyevalia shati jeupe lenye nukta nyeusi anashikilia kipaza sauti huku akiwa amesimama mbele ya mandhari nyeusi yenye alama nyeupe.\n",
      "Traduction: kijana aliyevalia suruali ya rangi ya kahawia na shati la kijani-kibichi anatazama juu ya ukuta wenye urefu wa kiuno unaotenganisha jikoni na chumba cha kuishi cha nyumba.\n",
      "Phrase: The people are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: A man is sleeping.\n",
      "Réponse: Mwanamume fulani amelala.\n",
      "Traduction: mtu amelala.\n",
      "Phrase: A man is outside.\n",
      "Réponse: Kuna mwanamume nje.\n",
      "Traduction: Kuna mwanamume nje.\n",
      "Phrase: There are multiple people present.\n",
      "Réponse: Kuna watu wengi waliopo.\n",
      "Traduction: Kuna watu wengi waliopo.\n",
      "Phrase: The man is sitting down.\n",
      "Réponse: Mwanamume huyo ameketi.\n",
      "Traduction: Mwanamume fulani ameketi.\n",
      "Phrase: Two dogs are playing.\n",
      "Réponse: Mbwa wawili wanacheza.\n",
      "Traduction: mbwa wawili wanacheza.\n",
      "Phrase: People are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: Bicyclists waiting at an intersection.\n",
      "Réponse: Waendesha baiskeli wakisubiri kwenye makutano.\n",
      "Traduction: Waendesha baiskeli wakisubiri kwenye makutano.\n",
      "Phrase: A woman is inside.\n",
      "Réponse: Kuna mwanamke ndani.\n",
      "Traduction: Kuna mwanamke ndani.\n",
      "Phrase: A man is sitting down.\n",
      "Réponse: Mwanamume fulani ameketi.\n",
      "Traduction: Mwanamume fulani ameketi.\n",
      "Phrase: The man is standing.\n",
      "Réponse: Mwanamume huyo amesimama.\n",
      "Traduction: Mwanamume huyo amesimama.\n",
      "Phrase: There are people outside.\n",
      "Réponse: Kuna watu nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: The people are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n",
      "Phrase: The people are outside.\n",
      "Réponse: Watu wako nje.\n",
      "Traduction: Watu wako nje.\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for i in range(20):\n",
    "    sentence = test_data[i][\"en\"]\n",
    "    expected_translation = test_data[i][\"ye\"]\n",
    "    translation = translate_sentence(sentence, model, en_vocab, ye_vocab, device)\n",
    "    predictions.append(translation)\n",
    "    references.append(expected_translation)\n",
    "    print(\"Phrase:\", sentence)\n",
    "    print(\"Réponse:\", expected_translation)\n",
    "    print(\"Traduction:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\PC\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bleu\\9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Tue Feb 25 07:41:18 2025) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mwanamume huyo ameketi.', 'Mwanamume huyo ameketi.')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0], references[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = nlp.encode(s).tokens\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = True\n",
    "tokenizer_fn = get_tokenizer_fn(ye_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.6867548255809744,\n",
       " 'precisions': [0.8066666666666666,\n",
       "  0.7153846153846154,\n",
       "  0.6545454545454545,\n",
       "  0.5888888888888889],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.056338028169014,\n",
       " 'translation_length': 150,\n",
       " 'reference_length': 142}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface utilisateur avec Streamlit\n",
    "st.title(\"Chatbot de Traduction Anglais → Swahili\")\n",
    "st.write(\"Entrez une phrase en anglais et obtenez la traduction en Swahili.\")\n",
    "\n",
    "# Saisie utilisateur\n",
    "sentence = st.text_input(\"Entrez votre texte en anglais :\", \"\")\n",
    "\n",
    "if st.button(\"Traduire\"):\n",
    "    if sentence:\n",
    "        translation = translate_sentence(sentence)\n",
    "        st.success(f\"**Traduction en Swahili :** {translation}\")\n",
    "    else:\n",
    "        st.warning(\"Veuillez entrer une phrase en anglais.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_nlpevn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
